# Local AI Chat on Android ğŸš€

Ce projet est une preuve de concept (Proof-of-Concept) dÃ©montrant le dÃ©ploiement d'un grand modÃ¨le de langage (LLM) en environnement 100% local sur un appareil Android, servi par une interface web construite avec Streamlit.

L'objectif principal Ã©tait un dÃ©fi personnel et d'apprentissage : explorer la faisabilitÃ© et les performances d'une pile IA complÃ¨te dans un environnement non conventionnel et Ã  ressources limitÃ©es, sans aucune dÃ©pendance au cloud.

![image](https://github.com/djaouad-ch/local-ai-chat-android/assets/103175782/25a1762c-da0f-4886-9ac6-857a2569fcc4)

<!-- NOTE POUR TOI : Remplace le lien de l'image ci-dessus par une vraie capture d'Ã©cran de ton application en fonctionnement ! C'est trÃ¨s important. -->

## ğŸ¯ Contexte et Objectif

En tant que dÃ©veloppeur en reconversion dans l'IA, j'ai voulu dÃ©mystifier le dÃ©ploiement de modÃ¨les et comprendre l'ensemble de la chaÃ®ne, du serveur d'infÃ©rence Ã  l'interface utilisateur. PlutÃ´t que d'utiliser des services cloud, j'ai choisi de relever le dÃ©fi de tout faire fonctionner sur mon tÃ©lÃ©phone Android pour maÃ®triser les contraintes de performance, de stockage et de configuration dans un environnement Linux embarquÃ© (via Termux).

Ce projet met en Ã©vidence les compÃ©tences suivantes :

- **IntÃ©gration de systÃ¨mes :** Assemblage et configuration de composants hÃ©tÃ©rogÃ¨nes (Ollama, Streamlit, Python) pour crÃ©er une application fonctionnelle.
- **DÃ©ploiement en environnement contraint :** Gestion des ressources (RAM, CPU) sur un appareil mobile.
- **RÃ©solution de problÃ¨mes :** Surmonter les dÃ©fis liÃ©s Ã  l'architecture aarch64 et Ã  l'Ã©cosystÃ¨me Termux/proot-distro.
- **Full-Stack IA :** ComprÃ©hension du flux de donnÃ©es depuis l'interaction utilisateur jusqu'Ã  la rÃ©ponse du modÃ¨le.

## âš™ï¸ Architecture et Stack Technique

L'application fonctionne en suivant ce schÃ©ma simple :

`[Utilisateur sur Android] -> [Interface Web Streamlit] -> [Serveur Ollama local] -> [ModÃ¨le Gemma 3]`

- **Serveur d'infÃ©rence :** [Ollama](https://ollama.com/) pour servir le modÃ¨le de langage localement.
- **Interface Web (Frontend) :** [Streamlit](https://streamlit.io/) pour crÃ©er rapidement une interface de chat interactive.
- **ModÃ¨le de Langage (LLM) :** `gemma3:270m`, un modÃ¨le lÃ©ger et performant adaptÃ© Ã  une exÃ©cution locale.
- **Langage de "glue" :** Python 3.10 pour le script Streamlit et les appels Ã  l'API d'Ollama.
- **Environnement d'exÃ©cution :** [Termux](https://termux.dev/) avec une distribution Ubuntu 22.04 (via `proot-distro`) sur Android 12.

## ğŸ› ï¸ Guide d'Installation

Ce guide suppose que vous avez dÃ©jÃ  installÃ© Termux et une distribution Ubuntu via `proot-distro`.

### 1. PrÃ©requis dans Ubuntu

Assurez-vous que les outils de base sont installÃ©s :
```bash
apt update && apt upgrade -y
apt install -y python3.10 python3.10-venv curl git
```

### 2. Installation d'Ollama

Suivez les instructions officielles pour une installation manuelle sur Linux ARM64 :

```bash
# Lancer le script d'installation
curl -fsSL https://ollama.com/install.sh | sh

# DÃ©marrer le serveur Ollama dans un terminal sÃ©parÃ© (ou avec tmux/screen)
ollama serve
```

### 3. TÃ©lÃ©chargement du ModÃ¨le

Dans un autre terminal, tÃ©lÃ©chargez le modÃ¨le Gemma 3 :

```bash
ollama pull gemma3:270m
```

### 4. Mise en Place du Projet

Clonez ce dÃ©pÃ´t et prÃ©parez l'environnement virtuel Python :

```bash
# Clonez le projet
git clone https://github.com/TON-NOM-UTILISATEUR/TON-NOM-DE-PROJET.git
cd TON-NOM-DE-PROJET

# CrÃ©ez et activez l'environnement virtuel
python3.10 -m venv venv310
source venv310/bin/activate

# Installez les dÃ©pendances
pip install --no-cache-dir -r requirements.txt
```

> **Note** : Le fichier `requirements.txt` doit contenir les lignes suivantes :
> ```
> streamlit
> requests
> ```

---

## ğŸš€ Lancement de l'application

Une fois que le serveur Ollama est en cours d'exÃ©cution, lancez l'application Streamlit :

```bash
# Assurez-vous que votre venv est activÃ©
source venv310/bin/activate

# Lancez l'application
streamlit run app.py
```

Ouvrez ensuite votre navigateur sur l'adresse IP locale et le port indiquÃ©s par Streamlit (gÃ©nÃ©ralement http://127.0.0.1:8501).

---

## ğŸ—ºï¸ Prochaines Ã‰tapes (Roadmap)

- ImplÃ©menter le streaming de la rÃ©ponse token par token.
- Ajouter un bouton "ArrÃªter la gÃ©nÃ©ration".
- Afficher un Ã©tat "occupÃ©" pendant que le modÃ¨le rÃ©flÃ©chit.
- GÃ©rer les erreurs de connexion au serveur Ollama.
- Permettre la sÃ©lection du modÃ¨le depuis l'interface.

---

## ğŸ“„ Licence

Ce projet est distribuÃ© sous la licence MIT. Voir le fichier LICENSE pour plus de dÃ©tails.